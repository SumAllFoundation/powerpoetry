{
 "metadata": {
  "name": "Powerpoetry Analysis"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Powerpoetry Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This analysis attempts to measure the progress of Powerpoetry users, an online poetry platform, along several dimensions including rhyming, ngrams and word frequencies such as positive/negative sentiment or use of abstract words and political words among others. The data analyzed contains 128000 poems spanning from Aug-2012 to Feb-2014, during which the platform has experienced an exponential growth path.\n",
      "\n",
      "We see some evidence that the poets use more rhyming in their poems more as they progress, as evidenced from the beta distribution of slant rhyme frequency. There is also some evidence to suggest that poets make less spelling mistakes, use more abstract words, and utilize more words connoting overconfidence. Less grammatical errors is certainly something desirable. An increase in frequency of abstract words might indicate an increasing sophistication in poetic skills. In addition, an increase in the overconfidence metric might suggest that the poets become more confident about their feelings and express them more explicitly as they become more proficient. Because the analysis targets return users, which is a small subset of entire user base, the findings might not generalize well onto the larger population. \n",
      "\n",
      "Secondly, we ask the question whether poems posted from richer neighborhoods show higher literacy characteristics compared to the ones from poorer neighborhoods. We utilize Powerpoetry data that includes user location information on the ZIP code level. We enrich the dataset by appending Census demographics data by Tract Level to approximate the income levels for the given user. The analysis suggests that higher income levels might be associated with better literacy skills. There are two caveats associated with this finding. First, the zip-code / tract-level location might not be granular enough and come with a high variance in terms of income levels. Thus, the median income level might not be a good representation for the income level of given poet. Secondly, trigram frequency might not be the best metrics to measure the language sophistication. In the future iterations, we might need to design a more refined metric to measure the literacy levels given the poem. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import pandas as pd\n",
      "from itertools import *\n",
      "from matplotlib import cm\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "import statsmodels.api as sm\n",
      "import matplotlib\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython.core.pylabtools import figsize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poetry_features = pd.read_csv('data/poetry_features.csv')\n",
      "#Dropping the missing data\n",
      "poetry_features = poetry_features.dropna(axis=0)\n",
      "poetry_features = poetry_features.drop(['punctFreq','alliterFreq','unigramFreq'],axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Powerpoetry has experienced a substantial jump in traffic with 10000 poems on average added to the website on a monthly basis in latter part of 2013."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import datetime\n",
      "dt = [datetime.utcfromtimestamp(poetry_features.created.iloc[n]) for n,line in enumerate(poetry_features.created)]\n",
      "pi = pd.PeriodIndex([pd.Period(d,'D') for d in dt])\n",
      "monthly_ = pd.DataFrame(np.ones(len(pi)),index=pi).resample('M',how='sum')\n",
      "\n",
      "ax = monthly_.plot(kind='bar',color=[\"#7A68A6\"],label='test',alpha=0.7)\n",
      "ax.set_ylabel('Number of Poems')\n",
      "ax.set_xlabel('Month')\n",
      "ax.set_title('Number of Poems posted on the Powerpoetry website')\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "\n",
      "Here is a brief description for each variable in our feature space. The data is available upon request.\n",
      "\n",
      "Rhyming:\n",
      "\n",
      "     'perfectRhymeFreq': Frequency of perfect rhyming in the poem.\n",
      "    \n",
      "     'slantRhymeFreq' : Frequency of slant rhyming in the poem.\n",
      "    \n",
      "     'alliterFreq': Frequency of alliteration rhyming in the poem.\n",
      " \n",
      " Word Frequencies:\n",
      " \n",
      " Please refer to [Harvard General Inquirer](http://www.wjh.harvard.edu/~inquirer/)  for more details.\n",
      "\n",
      "     'ABS' : Frequency of abstract words in the poem.\n",
      "    \n",
      "     'EnlTot',: Frequency of words referring to knowledge, insight, and information concerning personal and cultural relations.\n",
      "    \n",
      "     'Female': Frequency of words referring to women and social roles associated with women\n",
      "    \n",
      "     'Male' : Frequency of words referring men to  and social roles associated with men\n",
      "    \n",
      "     'Object': References to objects\n",
      "    \n",
      "     'Polit': Freqency of words having a clear political character, including political roles, collectivities, acts, ideas, ideologies, and symbols.\n",
      "    \n",
      "     'Race': Frequency of words referring to racial or ethnic characteristics.\n",
      "    \n",
      "     'Relig': Words pertaining to religious, metaphysical, supernatural or relevant philosophical matters.\n",
      "    \n",
      "     'St': Word connoting overstated / understated expressions\n",
      "    \n",
      "     'WlbPhycs': Words connoting the physical aspects of well being, including its absence.\n",
      "    \n",
      "     'WlbPsyc': Words connoting the psychological aspects of well being, including its absence.\n",
      "    \n",
      "     'PosNeg': 'Words with positive and negative outlook'\n",
      "\n",
      "Ngrams: \n",
      "\n",
      "[COCA](http://corpus.byu.edu/) Corpus was used in the analysis.\n",
      "\n",
      "     'unigramFreq': Average occurrence of a word in the poem in the English language in terms of percentage.\n",
      "    \n",
      "     'bigramFreq' : Log average occurrence count of combination of two words in the poem in the English language\n",
      "    \n",
      "     'trigramFreq': Log average occurrence count of combination of three words in the poem in the English language\n",
      "    \n",
      "     'misspeltWord': Frequnecy of misspelt words in the poem.\n",
      "    \n",
      "     'sentence_count': Sentence count in the poem.\n",
      "    \n",
      "     'wordCount': Log word count in the poem.\n",
      "    \n",
      "     'punctFreq': Punctutation frequency in the poem.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of our takeaways in the process was the prevalence of dumping behavior for most Powerpoetry users, which is defined as posting multiple poems in a short time frame. We think that the users post lot of poems at once to increase their chances in competitions that occasionally take place on the website. Because the progress in the analysis is measured over sequence of poems posted, we want to control for this behavior by taking the mean of multiple entries on the same day. Although this is not the perfect way to correct for the bias, we think that it would still allow to measure progress.\n",
      "\n",
      "The code below achieves this correction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Daily Period Index\n",
      "poetry_features['periodindex'] = pd.PeriodIndex([pd.Period(d,'D') for d in dt])\n",
      "\n",
      "#Group By user id and PeriodIndex. Take the Mean to take into account dumping behavior.\n",
      "grouped = poetry_features.groupby(['uid','periodindex']).aggregate(np.mean).sort_index()\n",
      "ut = grouped.index\n",
      "\n",
      "#Reindex to count from date \n",
      "count_ = pd.DataFrame(grouped.index.tolist())[0].value_counts().sort_index()\n",
      "count = [i+1 for user,x in count_.iteritems() for i in range(x)]\n",
      "\n",
      "#Create indexes with user id, period and poem number to be used later in the analysis>\n",
      "\n",
      "nix = [(u[0],u[1],c)for c,u in zip(count,ut)]\n",
      "nth =  [x[2] for x in nix]\n",
      "uids = [x_[0] for x_ in nix]\n",
      "\n",
      "grouped.index = uids #Change the index to user IDs.\n",
      "\n",
      "\n",
      "#Clean nonrequired columns and admin uids. \n",
      "def clean(df):\n",
      "\t#Clean. #Drop User == 1 and 0 and 4. These users represent admin. \n",
      "\ttry:\n",
      "\t\tdf = df.drop(['created','nid'],axis=1)\n",
      "\t\tdf = df.drop([(0,),(1,),(4,)])\n",
      "\texcept:\n",
      "\t\tpass\n",
      "\treturn df\n",
      "\n",
      "grouped = clean(grouped)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we normalize the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grouped = (grouped - grouped.mean()) / grouped.std()\n",
      "grouped.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Number of poems used in the analysis before/after dumping behavior."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Number of poems {0} on the website'.format(len(poetry_features))\n",
      "print 'Number of poems {0} on the website after dumping behavior is taken into account'.format(len(grouped))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Number of poets posted on the website,\n",
      "print 'Number of poets {0} on the website'.format(len(set(poetry_features.uid)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start with some explanatory analysis. First chart shows the number of return poems on the site on a monthly basis. The goal of measuring the user progress puts the user profile in the heart of our analysis. There seems to be approximately 1500 return users on a monthly basis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Count the number of returns\n",
      "return_traffic = pd.DataFrame(map((lambda x: (x[1], int(x[2] > 1))),nix)).groupby(0).sum().resample('M',how='sum')\n",
      "#Count the number of first timers.\n",
      "#first_time_traffic = pd.DataFrame(map((lambda x: (x[1], int(x[2] == 1))),nix)).groupby(0).sum().resample('M')\n",
      "#Merge\n",
      "traffic = pd.concat([return_traffic],axis=1)\n",
      "traffic.columns = ['return']\n",
      "\n",
      "ax = traffic.plot(kind='bar',color=[\"#7A68A6\"],label='test',alpha=0.7)\n",
      "ax.set_ylabel('Number of Visitors')\n",
      "ax.set_xlabel('Month')\n",
      "ax.set_title('Breakdown of return visitors to Powerpoetry website')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compared to number of first time poems, which seems to hover around 5000. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_time_traffic = pd.DataFrame(map((lambda x: (x[1], int(x[2] == 1))),nix)).groupby(0).sum().resample('M',how='sum')\n",
      "traffic = pd.concat([first_time_traffic],axis=1)\n",
      "traffic.columns = ['first time']\n",
      "\n",
      "ax = traffic.plot(kind='bar',color=[\"#7A68A6\"],label='test',alpha=0.7)\n",
      "ax.set_ylabel('Number of Visitors')\n",
      "ax.set_xlabel('Month')\n",
      "ax.set_title('Breakdown of first time visitors to Powerpoetry website')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's also see the distribution of number of poems submitted for each user. The distribution does exhibit log-log charactertics. One user singlehandidly has posted 377 poems on the website. The chart also makes it clear that overwhelming number of poets posted only only one poem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nthpoem = np.array([i[2] for i in nix])\n",
      "bin_counts_cumulative = np.bincount(nthpoem)[1:][::-1] #Reverse Order\n",
      "bin_counts = [bin_count - bin_counts_cumulative[i-1] for i, bin_count in enumerate(bin_counts_cumulative) if i > 0] #Skip the first number\n",
      "bin_counts = [bin_counts_cumulative[0]] +  bin_counts #Append the first entry back\n",
      "bin_counts.reverse() #Reverse\n",
      "ind  = range(1,1 + len(bin_counts)) #the x Locations for the groups\n",
      "\n",
      "for i, n in enumerate(bin_counts):\n",
      "\tif i < 20:\n",
      "\t\tprint 'number of poets who posted {0} poem(s) : {1}'.format(i,n)\n",
      "#Chart the distribution\n",
      "fig, ax = plt.subplots()\n",
      "ax.bar(ind,bin_counts,color=[\"#7A68A6\"],alpha=0.7)\n",
      "ax.set_ylabel('Number of poets')\n",
      "ax.set_xlabel('Number of poems')\n",
      "ax.set_title('Distribution of Activity of Poets')\n",
      "ax.annotate('Hello most active user !', xy=(ind[-1], 1), xytext=(300, 20000),\n",
      "            arrowprops=dict(facecolor='black', shrink=0.05))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Progress"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below we will measure the language progress in three dimensions. First, we will calculate the progress for each user in each feature and look at the distribution of betas. A frequency distribution with a mean that is statistically significant from zero might imply progress on average given the particular feature. Secondly, we will take the median score for each poem in the sequence (1st poem, 2nd poem etc) and fit a linear model. A slope that is significant and non-zero might be a sign that the users show some improvement on average. Third, we will employ a A/B test to discern the difference between first set of poems and latter set of poems.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "(i) Distribution of Betas\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the analysis we only consider poets who posted at least 10 poems. We then measure the beta for each user in each feature (trigram, bigram, PosNeg etc) and analyze the distribution for each feature. \n",
      "\n",
      "There are 154 poets with at least 10 poems. We later will see that the result would differ with different cutoff points. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def seasoned(cut,nix,grouped):\n",
      "\tix = filter((lambda x: x[2] >=cut),nix) #seasoned poets with at least 10 poems posted\n",
      "\tunique_uids = np.unique([i[0] for i in ix])\n",
      "\tunique_nids = [ix_[2] for ix_ in nix if ix_[0] in unique_uids] #All the nids for seasoned\n",
      "\tfeatures_ = grouped.loc[unique_uids] #Feature set for seasoned poets.\n",
      "\t#print len(unique_uids)\n",
      "\treturn(features_, unique_nids, unique_uids)\n",
      "\n",
      "\n",
      "def distribution_betas(seasoned_poets,unique_uids):\n",
      "\t'Looping through the dataframe to retrieve the scores for each user at a time and calculate the progress'\n",
      "\tsig = 0\n",
      "\tbetas_user = np.zeros((len(unique_uids),seasoned_poets.shape[1])) \n",
      "\tfor x,uid in enumerate(unique_uids):\n",
      "\t\tscores = seasoned_poets.loc[uid]\n",
      "\t\t#Iterate over the score\n",
      "\t\tfor z,(feature,score) in enumerate(scores.iteritems()):\n",
      "\t\t\t#Regression coefficients\n",
      "\t\t\ty = score.values\n",
      "\t\t\tX = arange(len(score))+1\n",
      "\t\t\tX = sm.add_constant(X)\n",
      "\t\t\tres = sm.OLS(y,X).fit() #OLS\n",
      "\t\t\t#if x < 1: #Show for the first user\n",
      "\t\t\t#\tprint feature, uid\n",
      "\t\t\t#\tprint res.summary()\n",
      "\t\t\t# \n",
      "\t\t\tbetas_user[x,z] = res.params[1]\n",
      "\t\t\tif res.f_pvalue<0.05: #Take the beta with a p-value threshold! \n",
      "\t\t\t\tsig += 1\n",
      "\tprint '{0} significant results out of {1} regressions'.format(sig,x* z)\n",
      "\n",
      "\t#Chart Distribution of Betas\n",
      "\tfigsize(25, 10)\n",
      "\tfig = plt.figure()\n",
      "\t#Subplots within a single chart\n",
      "\tbetas_, columns_ = betas_user.T, seasoned_poets.columns\n",
      "\tfor k, (beta, column) in enumerate(zip(betas_,columns_)):\n",
      "\t\t    sx = plt.subplot(int(betas_.shape[0]/2+1), 2, k+1)\n",
      "\t\t    plt.rc('axes', color_cycle=['r', 'g', 'b', 'y'])\n",
      "\t\t    #plt.xlabel(seasoned_poets.columns[k])\n",
      "\t\t    #Label\n",
      "\t\t    plt.text(beta.min(), 40, 'average beta is {0}'.format(round(beta.mean(),2)), fontsize=15) \n",
      "\t\t    plt.setp(sx.get_yticklabels(), visible=False)\n",
      "\t\t    plt.hist(beta,color=cm.jet(1.*k/len(betas_)), alpha=0.4, bins= np.linspace(beta.min(), beta.max(), 20)) #beta is the all the betas for all the users for each matric.\n",
      "\t\t    plt.ylim(0,50)\n",
      "\t\t    plt.legend([column])\n",
      "\t\t    plt.vlines(0, 0, 500, color=\"k\", linestyles=\"--\", lw=1)\n",
      "\t\t    #Significance test\n",
      "\t\t    p_val = stats.ttest_1samp(beta, 0)[1]\n",
      "\t\t    if p_val < 0.10: plt.text(beta.min(), 30, 'statistically significant at {0}'.format(round(p_val,3)), fontsize=15) \n",
      "\t\t    #plt.autoscale(tight=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cut = 10\n",
      "seasoned_poets, unique_nids,unique_uids = seasoned(cut,nix,grouped) #Returns poems from poets with more than n poems. UID is the index\n",
      "distribution_betas(seasoned_poets,unique_uids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that around 7 percent of the betas are significant with a minimum number of 10 poems! Few things to note here. First, keep in mind that the features are normalized; so betas are somewhat comparable. Secondly, while for some measures positive beta translates to improvement, for others the oppositive is true. For example, we associate a descrease in trigram frequencies with improvement. The assumption is that the user is able to come up with unexpected combination of words as the languge skills improve. \n",
      "\n",
      "We perform a simple t-test to see whether the progress beta is statistically significantly different from zero. Although the mean betas are mostly close to zero, we see that poets rhyme their poems more as they progress, as evidenced from the beta distribution of slant rhyme frequency. Secondly, we see that the frequency of abstract words increases as poets post more. This is also significant but at 10% level. \n",
      "\n",
      "There is really no real reason to use at least 10 poems. If we extend the analysis onto different cutoff points (11,12,..) we see that ST and freqeuncy of misspelt words also show improvement. That is to say that poets make less spelling mistakes and utilize more words connoting overconfidence. Less grammatical errors is certainly something desirable. In addition, an increase in the ST metric might suggest that the poets become more confident about their feelings and express them more explicitly as they become more proficient."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cut = 12\n",
      "seasoned_poets, unique_nids,unique_uids = seasoned(cut,nix,grouped) #Returns poems from poets with more than n poems. UID is the index\n",
      "distribution_betas(seasoned_poets,unique_uids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "(ii) Linear Progress\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we move to the estimation of the progression slope for each feature on average. This is done by taking the median score for nth poem across all poets before applying a linear fit. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def abbreviate(seasoned_poets,cut):\n",
      "\tseasoned_poets_abbreviated = seasoned_poets.copy()\n",
      "\tseasoned_poets_abbreviated.index = unique_nids\n",
      "\tseasoned_poets_abbreviated['nid'] = unique_nids\n",
      "\t#Filter by cutoff point. Take only nth poems up to cut\n",
      "\tfilter_ =  seasoned_poets_abbreviated['nid']<=cut\n",
      "\tseasoned_poets_abbreviated = seasoned_poets_abbreviated[filter_] #Take only the first 'cut' poems\n",
      "\tseasoned_poets_abbreviated = seasoned_poets_abbreviated.drop('nid',axis=1)\n",
      "\treturn seasoned_poets_abbreviated\n",
      "\n",
      "def average_progress_chart(seasoned_poets_abbreviated,unique_nids):\n",
      "\tfigsize(25, 10)\n",
      "\t#print seasoned_poets_abbreviated, ' poets in total'\n",
      "\tbetas_median_feature = np.zeros(seasoned_poets_abbreviated.shape[1])\n",
      "\tfig = plt.figure()\n",
      "\tsubplots_adjust(hspace=0.,wspace=0.) \n",
      "\tfor k,(column,feature) in enumerate(seasoned_poets_abbreviated.iteritems()):\n",
      "\t\t#Plot params\n",
      "\t\tsx = plt.subplot(int(betas_median_feature.shape[0]/2+1), 2, k+1)\n",
      "\t\tplt.rc('axes', color_cycle=['r', 'g', 'b', 'y'])\n",
      "\t\tplt.setp(sx.get_yticklabels(), visible=False)\n",
      "\t\t#Median for each poem number first\n",
      "\t\t#print column\n",
      "\t\tmedian_feature = feature.groupby(feature.index).median()\n",
      "\t\ty = median_feature.values\n",
      "\t\tX = median_feature.index\n",
      "\t\tX = sm.add_constant(X)\n",
      "\t\tres = sm.OLS(y,X).fit()\n",
      "\t\tbetas_median_feature[k] = res.params[1]\n",
      "\t\t#Plot the Progress\n",
      "\t\tplt.setp(sx.get_yticklabels(), visible=False)\n",
      "\t\tplt.text(1, y.max(), 'beta of the slope is {:10.4f} with p-value of {:10.4f}'.format(res.params[1],res.f_pvalue), fontsize=12) \n",
      "\t\t\n",
      "\t\t#Scatter\n",
      "\t\tsx = plt.scatter(X[:,1],y, alpha=0.9,marker=(6,0))\n",
      "\t\t#Line\n",
      "\t\tx_ = np.unique(X[:,1])\n",
      "\t\ty_ = (res.params[0] + res.params[1]*np.unique(X[:,1]))\n",
      "\t\t#Fit the Line of the regression\n",
      "\t\tcol = cm.jet(1.*k/len(betas_median_feature))\n",
      "\t\tplt.plot(x_,y_,color=col)\t\n",
      "\t\tplt.legend([column])\n",
      "\t\txlim(0,cut)\n",
      "\t\tplt.autoscale(tight=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, the choice of the cutoff point is somehow arbitrary. We face a trade-off. The higher the number, the longer we can extend the progress (on to 12th poem, 13th poem etc) but risk losing poets who have not posted the required minimum and hence a smaller number of poets who qualify. Since the distribution of poem count is highly skewed, a large cutoff number increases the risk of the result not generalizing well to the entire population. Here is a graph showing the tradeoff."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npoems = []\n",
      "cutoff = []\n",
      "for i in range(8,30):\n",
      "\tseasoned_poets, unique_nids,unique_uids = seasoned(i,nix,grouped)\n",
      "\tseasoned_poets_abbreviated = abbreviate(seasoned_poets,i)\n",
      "\tcutoff.append(i)\n",
      "\tnpoems.append(seasoned_poets_abbreviated.shape[0]/i)\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(cutoff,npoems)\n",
      "ax.set_ylabel('Number of poets with at least minimum number of poems')\n",
      "ax.set_xlabel('Number of poems minimum')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, arbitrarily, setting the minimum number of poems at 12 looks plausible. Because there are still 100 poems who posted more than the required minimum and also an inference can be made using 12 points in a linear setting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = 12\n",
      "seasoned_poets, unique_nids,unique_uids = seasoned(i,nix,grouped)\n",
      "seasoned_poets_abbreviated = abbreviate(seasoned_poets,i)\n",
      "average_progress_chart(seasoned_poets_abbreviated,unique_nids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In lower cutoff points (i=12), we see that the users take strides in expressing themselves as ST feature (frequenct of overstated/understated expression) increases. We also find that the users seem to be using more abstract words as they post more poems.\n",
      "\n",
      "With a large required minimum (and fewer poets), we see that sentence count and frequency of political words show an increase. This might show us that users have more things to say as they write and they become political in doing so. Additionaly, using the mean score instead of median score reveals that the positivity of the poems on average improve with the poem count.\n",
      "\n",
      "On the other hand, we see a detoriation in the frequency of trigrams and bigrams in general. This is opposite of what we would have expected. It could be that our COCA dictionary is failing to pick up combination of less frequent words. In fact, we might need to run the numbers again to incorporate the latest version of COCA, which is much more comprehensive that the previous one that we used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = 20\n",
      "seasoned_poets, unique_nids,unique_uids = seasoned(i,nix,grouped)\n",
      "seasoned_poets_abbreviated = abbreviate(seasoned_poets,i)\n",
      "average_progress_chart(seasoned_poets_abbreviated,unique_nids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "(iii) A/B Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another approach is to divide the progress into two (or more) subsets in order to see if the latter sample is significantly different than the former one. If the means of two samples are different significantly, we can argue that the poets display *different* characteristics with their latter poems.\n",
      "\n",
      "We use the non-parametric KOLMOGOROV-SMIRNOV (KS) test because the distribution of the feature is often not normal.Similar to above, the initial step is to take the poets who have posted at least a certain number of poems. Then we compare the first subset against the second subset. We keep the size of two samples equal, because KS test is sensitive to differences in sample size. \n",
      "\n",
      "The example below compares the first 6 poems vs second 6 poems. Both visually and statistically, it is hard to discern a difference between two samples. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ab_test(cut,nix,grouped):\n",
      "    seasoned_poets, unique_nids, unique_uids = seasoned(cut,nix,grouped)\n",
      "    seasoned_poets.index = unique_nids\n",
      "    print 'Total number of {0} poets'.format(len(unique_uids))\n",
      "    \n",
      "    \n",
      "    figsize(25, 10)\n",
      "    fig = plt.figure()\n",
      "    \n",
      "    n = cut//2 #Take the first set of poems.\n",
      "    for k,(column,feature) in enumerate(seasoned_poets.iteritems()):\n",
      "        a = feature[seasoned_poets.index<=n]\n",
      "        b = feature[seasoned_poets.index>n]\n",
      "        b = b[b.index<=n*2] #Keep the sample sizes equal\n",
      "        sx = plt.subplot(int(seasoned_poets.shape[1]/2+1), 2, k+1)\n",
      "        plt.setp(sx.get_yticklabels(), visible=False)\n",
      "    \n",
      "        lp = np.linspace(min(min(a),min(b)),max(max(a),max(b)),30)\n",
      "        plt.hist(a,bins=lp,alpha=0.5)\n",
      "        plt.hist(b,bins=lp,alpha=0.5)\n",
      "        plt.autoscale(tight=True)\n",
      "        plt.legend([column])\n",
      "    \n",
      "        #two_sample = stats.ttest_ind(a, b)\n",
      "        #test_stat = stats.ranksums(a, b) \n",
      "        test_stat = stats.ks_2samp(a, b)\n",
      "        print 'For the feature {2}: The t-statistic is {0} and the p-value is {1}.'.format(test_stat[0],test_stat[1],column)\n",
      "        \n",
      "#Analyze 12 poems - First 6 vs second 6.       \n",
      "ab_test(12,nix,grouped)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is also possible to change the number of poems that we want to compare also because the user on average might achieve progress at different points with different features. For example, a poet might start writing longer poems from 5th poem on but not make permanent gains in trigram frequencies until much later.\n",
      "\n",
      "Running the analysis with different cutoff points does not provide any useful insights, however. From an A/B test perspective, we cannot say that two samples look different from each other for any given feature."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ab_test(6,nix,grouped)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Income Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second part of this writeup analyzes literacy skills across different demographics, particularly the income level versus the trigram frequency. We take trigram frequency as a proxy for language mastery. Advanced language skills should enable the poet to string together unexpected combinations of words, hence result in a lower trigram frequeny. \n",
      "\n",
      "\n",
      "For each geographical region, we calculate the median value for each feature. We require a minimum number of poems for any given tract to avoid small sample bias. Then we run a linear regression to find the income - normalized trigram relationship. The charts illustrate the relationship. X-axis is the income, Y-axis is the normalized trigram frequency for the region.\n",
      "\n",
      "We notice that there is some downward trend in trigram frequency which is what we were hoping for."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = pd.read_csv('data/poetry_features.csv')\n",
      "\n",
      "features = features.drop(['punctFreq','alliterFreq','unigramFreq','Race'],axis=1)\n",
      "features.index = features['nid']\n",
      "features = features.drop(['nid','created','uid'],axis=1)\n",
      "\n",
      "\n",
      "tracts = pd.read_csv('data/location_tracts.csv')\n",
      "loc = tracts[['nid','AcsHouseholdIncomeMedian','TractCode']]\n",
      "loc.index = loc['nid']\n",
      "loc = loc.drop('nid',axis=1)\n",
      "features_with_loc = loc.join(features)\n",
      "features_with_loc = features_with_loc.dropna(axis=0)\n",
      "\n",
      "features_with_loc['trigramFreq'] = (features_with_loc['trigramFreq'] - features_with_loc['trigramFreq'].mean()) / float(features_with_loc['trigramFreq'].std())\n",
      "\n",
      "\n",
      "#Group By Tracts. #Take tracts with only at least 20 poems. 139 tracts.\n",
      "tract_size = features_with_loc.groupby('TractCode').size()\n",
      "tract_list = tract_size[tract_size>20]\n",
      "#Subset of feature set\n",
      "features_with_loc['take'] = features_with_loc['TractCode'].map(lambda x: x in tract_list)\n",
      "features_with_loc_subset = features_with_loc.dropna(axis=0)\n",
      "features_with_loc_subset = features_with_loc[features_with_loc['take']==True]\n",
      "\n",
      "#Group By TractCode. Take the median. 139 Tracts in general\n",
      "features_by_tract = features_with_loc_subset.groupby('TractCode').aggregate('median')\n",
      "features_by_tract = features_by_tract.drop(['take'],axis=1)\n",
      "features_by_tract['AcsHouseholdIncomeMedian'] = features_by_tract['AcsHouseholdIncomeMedian'].map(lambda x: int(x))\n",
      "\n",
      "\n",
      "trigram_by_tract = pd.concat([trigrams,features_by_tract['AcsHouseholdIncomeMedian']],axis=1)\n",
      "\n",
      "print 'Number of regions', len(trigram_by_tract)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def average_progress_chart(data):\n",
      "\tfigsize(20, 15)\n",
      "\n",
      "\tfig = plt.figure()\n",
      "\tsx = plt.subplot(1,1,1)\n",
      "\tplt.setp(sx.get_yticklabels(), visible=False)\n",
      "\n",
      "\n",
      "\t#Median for each poem number first\n",
      "\t#print feature\n",
      "\ty = data.values[:,0]\n",
      "\tX = data['AcsHouseholdIncomeMedian'].values\n",
      "\tX = sm.add_constant(X)\n",
      "\tres = sm.OLS(y,X).fit()\n",
      "\tbetas_median = res.params[1]\n",
      "\t#Plot the Progress\n",
      "\t#Scatter\n",
      "\tplt.scatter(X[:,1],y, alpha=0.9,marker=(6,0))\n",
      "\t#Line\n",
      "\tx_ = np.unique(X[:,1])\n",
      "\ty_ = (res.params[0] + res.params[1]*x_)\n",
      "\t#Fit the Line of the regression\n",
      "\tplt.plot(x_,y_,color='red')\t\n",
      "\tplt.legend([column])\n",
      "\t#xlim(0,cut)\n",
      "\tplt.autoscale(tight=True)\n",
      "\t#Linear Fit\n",
      "\tplt.text(x_.min(), y.max(), 'beta of the slope is {:10.4f} with p-value of {:10.4f}'.format(res.params[1],res.f_pvalue), fontsize=20)\n",
      "\t#Labels\n",
      "\tsx.set_ylabel('Trigram Frequencies')\n",
      "\tsx.set_xlabel('Income Levels')\n",
      "\tsx.set_title('Trigram Frequencies by Income Levels')\n",
      "\tprint res.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, the explanatory power of the slope is not very high. We might try to remove the points with high leverage from the dataset. When we do this, we get a better fit. Nonetheless, the downtrend in trigram frequency with higher income levels is visually observable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "average_progress_chart(trigram_by_tract)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trigram_by_tract_ = trigram_by_tract[trigram_by_tract.AcsHouseholdIncomeMedian<70000]\n",
      "average_progress_chart(trigram_by_tract_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last step is to run a simple two-sided t-test to see if the means of literacy skills for poets from high-income and low-income are different statistically. We use the median income level to split the data into two equal-sized components."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ab_test_income(data):\n",
      "\n",
      "    figsize(25, 10)\n",
      "    fig = plt.figure()\n",
      "    \n",
      "    median_income = trigram_by_tract.AcsHouseholdIncomeMedian.median()\n",
      "    a = trigram_by_tract['trigramFreq'][trigram_by_tract.AcsHouseholdIncomeMedian<=median_income]\n",
      "    b = trigram_by_tract['trigramFreq'][trigram_by_tract.AcsHouseholdIncomeMedian>median_income]\n",
      "\n",
      "\n",
      "\n",
      "    lp = np.linspace(min(min(a),min(b)),max(max(a),max(b)),10)\n",
      "    ax = plt.subplot(211)\n",
      "    plt.hist(a,bins=lp,alpha=0.5,color=\"#467821\")\n",
      "    plt.legend(['Lower Income'])\n",
      "    ax = plt.subplot(212)\n",
      "    plt.hist(b,bins=lp,alpha=0.5,color=\"#7A68A6\")\n",
      "    plt.autoscale(tight=True)\n",
      "    plt.legend(['Higher Income'])\n",
      "    \n",
      "\n",
      "    #two_sample = stats.ttest_ind(a, b)\n",
      "    #test_stat = stats.ranksums(a, b) \n",
      "    test_stat = stats.ks_2samp(a, b)\n",
      "    print 'The mean trigram frequency for poets from high-income neighborhoods is {0}'.format(a.mean())\n",
      "    print 'The mean trigram frequency for poets from high-income neighborhoods is {0}'.format(b.mean())\n",
      "    print 'For the feature {2}: The t-statistic is {0} and the p-value is {1}.'.format(test_stat[0],test_stat[1],column)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The chart below shows the trigram frequency distribution for each subgroup. A lower frequency indicates a higher sophistication. We find weak evidence that the poets from high-income areas have superior language skills as measured by trigram frequencies. There are two possible problems with this finding. First, the zip-code / tract-level location might not be granular enough and have a high variance attached in terms of income levels. Thus, the median income level might not be a good representation of the income level of given poet. Secondly, trigram frequency might not be the best metrics to measure the language sophistication. In the future iterations, we might need to design a more refined metric to measure the literacy levels given the poem. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ab_test_income(trigram_by_tract)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}